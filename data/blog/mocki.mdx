---
title: 'MOCKI â€“ AI-Powered Mock Interview Platform / MOCKI â€“ AI é©±åŠ¨çš„æ¨¡æ‹Ÿé¢è¯•å¹³å°'
date: '2025-04-04'
tags: ['MOCKI', 'RAG', 'Agent', 'LLM']
summary: 'MOCKI â€“ AI-Powered Mock Interview Platform / MOCKI â€“ AI é©±åŠ¨çš„æ¨¡æ‹Ÿé¢è¯•å¹³å°'
images: ['/static/images/mocki.png']
authors: ['default']
---

# MOCKI â€“ AI-Powered Mock Interview Platform

**Tech Stack**: React + FastAPI + MongoDB + OpenAI + Qdrant + Redis + Celery + WebRTC + Firebase Auth + Resend

## System Architecture

```mermaid
graph TD
    User[User] --> Frontend[Frontend React]
    Frontend --> WebRTC[WebRTC Audio/Video]
    Frontend --> Websocket[Websocket Real-time Communication]
    Frontend <--> FirebaseAuth[Firebase Auth Authentication]

    Websocket <--> NodeBackend[NestJS Backend]
    NodeBackend <--> RedisPS[Redis Pub/Sub]
    NodeBackend <--> PythonBackend[Python Backend]

    NodeBackend --> STT[Speech to Text STT]
    NodeBackend --> TTS[Text to Speech TTS]
    NodeBackend --> TranslationAPI[Translation API]

    PythonBackend --> OpenAI[OpenAI API]
    PythonBackend --> Qdrant[Qdrant Vector Database]
    PythonBackend --> MongoDB[MongoDB]
    PythonBackend --> Redis[Redis Cache]
    PythonBackend --> CeleryWorkers[Celery Workers]

    CeleryWorkers --> Flower[Flower Monitoring]
    Flower --> Prometheus[Prometheus]
    Prometheus --> Grafana[Grafana Visualization]

    PythonBackend --> Gmail[Gmail API]
    PythonBackend --> Resend[Resend Email Service]

    subgraph Security System
    RateLimiter[Redis Rate Limiter]
    BloomFilter[Bloom Filter]
    Vault[HashiCorp Vault]
    end

    PythonBackend --> Security System
```

## ğŸŒŸ Core Features & System Modules

### 1. AI-Powered Mock Interview Platform (Core Product Capability)

```mermaid
sequenceDiagram
    participant User
    participant Frontend
    participant NestJS
    participant PythonBackend
    participant OpenAI
    participant CacheSystem

    User->>Frontend: Initiate Interview Request
    Frontend->>NestJS: WebRTC Audio/Video Connection
    Frontend->>NestJS: Websocket Connection
    NestJS->>PythonBackend: Establish Websocket Connection

    loop Interview Process
        User->>Frontend: Audio Input
        Frontend->>NestJS: Audio Stream
        NestJS->>NestJS: STT Conversion
        NestJS->>PythonBackend: Text Question

        PythonBackend->>CacheSystem: Check Question Cache
        alt Cache Hit
            CacheSystem-->>PythonBackend: Return Cached Result
        else Cache Miss
            PythonBackend->>OpenAI: LLM Inference Request
            OpenAI-->>PythonBackend: Return Answer
            PythonBackend->>CacheSystem: Update Cache
        end

        PythonBackend->>NestJS: Return AI Answer
        NestJS->>NestJS: TTS Conversion
        NestJS->>Frontend: Audio Answer
        Frontend->>User: Play AI Answer
    end

    User->>Frontend: End Interview
    Frontend->>NestJS: Close Connection
    NestJS->>PythonBackend: Save Interview Record
```

- Independently developed complete pipeline: audio-video interview process based on WebRTC â†’ STT â†’ LLM inference â†’ TTS audio return, supporting seamless switching between AI and human interviewers
- Built real-time communication system based on Socket.IO + Redis Pub/Sub + Redis Adapter, supporting message synchronization and state broadcasting in multi-client, multi-instance deployments
- Supports role switching (system, AI, human), question bank management, resume import, and position-specific Q&A, building structured and unstructured question combinations
- System Architecture:
  - NestJS Backend: Handles audio-video calls and conversation records, HTTP calls to translation and TTS APIs, streaming STT (Baidu, Google, iFlytek), maintains WebSocket connections with frontend and Python backend
  - Python Backend: Handles AI requests, RAG system, email analysis system, user login management, rate limiting. Connects with NodeJS via WebSocket in AI interview system, and interacts with frontend via REST API for login and email analysis

### 2. Intelligent Email Analysis System (AI Information Extraction + Auto-closing)

```mermaid
graph TD
    Start[User Authorization] --> OAuth[Gmail OAuth2.0 Authentication]
    OAuth --> FetchEmails[Fetch Application Emails]
    FetchEmails --> Filter[Filter by Labels/Time]

    Filter --> ParallelProcessing[Parallel Async Processing]
    ParallelProcessing --> CeleryTasks[Celery Task Distribution]

    CeleryTasks --> AnalyzeEmail1[Analyze Email 1]
    CeleryTasks --> AnalyzeEmail2[Analyze Email 2]
    CeleryTasks --> AnalyzeEmailN[Analyze Email N...]

    AnalyzeEmail1 --> LLMProcessing[LLM Information Extraction]
    AnalyzeEmail2 --> LLMProcessing
    AnalyzeEmailN --> LLMProcessing

    LLMProcessing --> ExtractStatus[Extract Application Status]

    ExtractStatus --> CreateTimeline[Generate Timeline]
    CreateTimeline --> GenerateTasks[Generate Tasks]
    CreateTimeline --> JobRecommendation[Job Recommendations]

    GenerateTasks --> Notification[Send Notifications]
    JobRecommendation --> Notification
```

- Supports user login via Gmail OAuth2.0, uses Gmail API to fetch application emails, filters historical emails by labels/time
- Uses parallel async LLM to analyze email content, extracts application status (read, replied, interview invitation), position name, company info, and aggregates into Timeline
- Integrates company position information to push job recommendations to users

### 3. Todo Management & Smart Reminder System

- Automatically generates todos based on email analysis and user behavior, "TODO" "DONE", building complete application tracking path
- Uses Resend to implement time-triggered deadline reminders, real-time notifications, and status change pushes, avoiding key information omissions
- Supports status synchronization and cross-device access, user operations instantly feedback to database

### 4. RAG System & Contextual Q&A Optimization

```mermaid
graph TD
    subgraph Index Construction
    Resume[User Resume] --> TextExtractor[Text Extraction]
    JobDesc[Job Description] --> TextExtractor
    TextExtractor --> Chunker[Text Chunking]

    Chunker --> QAPairCreator[Query-Answer Pair Generation]
    QAPairCreator --> QueryEmbedder[Query Embedding Generation]
    QAPairCreator --> AnswerEmbedder[Answer Embedding Generation]

    QueryEmbedder --> VectorStore[Qdrant Vector Storage]
    AnswerEmbedder --> VectorStore
    QAPairCreator --> MetadataStore[MongoDB Metadata]
    end

    subgraph Retrieval Process
    Query[User Question] --> InputEmbedder[Query Embedding Generation]

    InputEmbedder --> QuerySimilarity[Query Similarity Search]
    InputEmbedder --> AnswerSimilarity[Answer Similarity Search]

    QuerySimilarity --> Qdrant[(Qdrant\nVector Database)]
    AnswerSimilarity --> Qdrant

    QuerySimilarity --> WeightedMerger[Weighted Result Merging]
    AnswerSimilarity --> WeightedMerger
    WeightedMerger --> DocumentIDs[Return Similar Document IDs]

    DocumentIDs --> MongoDB[(MongoDB\nMetadata Storage)]
    MongoDB --> RetrievedDocs[Retrieve Complete Documents]
    end

    subgraph Context Management
    RetrievedDocs --> ContextFormatter[Context Formatting]
    ContextFormatter --> TokenCounter[Token Counter]

    PreviousQA[Previous Q&A] --> SummaryBuffer[Summary Buffer]
    SummaryBuffer --> TokenThreshold[Token Threshold Control]
    TokenThreshold --> ContextTruncation[Context Truncation]
    end

    subgraph Multi-Agent Routing
    Query --> IntentClassifier[Intent Classifier]
    IntentClassifier --> MemoryRouter[Memory Router]
    MemoryRouter --> BQAgent[BQ Interview Agent]
    MemoryRouter --> CodeAgent[Coding Interview Agent]
    MemoryRouter --> KnowledgeAgent[Basic Knowledge Agent]
    MemoryRouter --> AnalysisAgent[Interview Analysis Agent]
    MemoryRouter --> QuestionAgent[Question Agent]
    end

    ContextFormatter --> LLMPrompt[LLM Prompt Construction]
    ContextTruncation --> LLMPrompt
    MemoryRouter --> LLMPrompt
    LLMPrompt --> OpenAI[OpenAI API]
    OpenAI --> Response[Generate Response]
```

- Vector search engine based on LangChain + Qdrant, supporting Query-Answer vector similarity search
- Efficient retrieval process: user questions are vectorized and simultaneously compared with pre-processed Query and Answer vectors for similarity calculation, with weighted merging of results to obtain the most relevant content
- Optimized document processing: interview materials are pre-processed into Query-Answer pairs, with separate embedding calculations, enabling retrieval to consider both question similarity and answer relevance
- Simplified index structure: direct vector similarity sorting for retrieval, avoiding complex index algorithm tuning while meeting performance requirements at current data scale
- Multi-turn Q&A using ConversationSummaryBufferMemory for summary caching, combined with Token threshold controller for automatic context truncation, reducing overall Token cost by 45%
- Intent classification using LangChain LLMChain, driving MemoryRouter for dynamic context switching between multiple Agents
- Professional Agent system includes:
  - BQ Interview Agent: handling behavioral questions and soft skills assessment
  - Coding Interview Agent: evaluating programming ability and algorithm analysis
  - Basic Knowledge Agent: testing professional domain knowledge
  - Interview Analysis Agent: providing interview performance evaluation and feedback
  - Question Agent: generating in-depth questions based on candidate background

### 5. Asynchronous Task System (Performance Guarantee)

```mermaid
graph LR
    subgraph Task Generation
    WebRequest[Web Request] --> TaskRouter
    ScheduledJob[Scheduled Job] --> TaskRouter
    EmailAnalysis[Email Analysis] --> TaskRouter
    end

    subgraph Task Routing
    TaskRouter[Task Router] --> PriorityQueue
    TaskRouter --> UserPriority{User Priority}
    UserPriority -->|VIP User| HighPriority[High Priority Queue]
    UserPriority -->|Regular User| NormalPriority[Normal Priority Queue]
    end

    subgraph Queue System
    PriorityQueue[Priority Queue] --> Redis[(Redis)]
    HighPriority --> Redis
    NormalPriority --> Redis
    end

    subgraph Worker Pool
    Redis --> Dispatcher[Task Dispatcher]
    Dispatcher --> CPUWorkers[CPU-intensive Workers\nPDF/Email Parsing]
    Dispatcher --> IOWorkers[IO-intensive Workers\nLLM Calls/Embedding Generation]
    end

    subgraph Monitoring System
    CPUWorkers --> Flower[Flower Monitoring]
    IOWorkers --> Flower
    Flower --> Prometheus[Prometheus]
    Prometheus --> Grafana[Grafana]
    Grafana --> Alerts[Alert Notifications]
    end
```

- Multi-machine multi-queue task scheduling system based on Celery + Redis, with Worker pool independently deployed for CPU-intensive (PDF/email parsing) and IO-intensive (LLM calls, embedding generation) tasks
- Priority task scheduling (e.g., VIP user request queue jumping), reducing urgent task average latency by 60%
- Real-time monitoring with Flower, combined with Prometheus/Grafana for automatic alerts and visual tracking of task failure rates and queue backlog metrics

#### Flower Monitoring Tool Details

Flower is Celery's officially recommended task monitoring tool, suitable for **real-time monitoring of Celery task execution, Worker status, task queue backlog, etc.**. It can be understood as Celery's visual Dashboard + API server.

**Flower Features:**

- ğŸ‘¨â€ğŸ”§ Worker status (active, stopped, heartbeat lost)
- ğŸ“¦ Queue length (backlog status)
- âœ… Task execution status (success, failure, retry, duration)
- ğŸ§© Task details (input parameters, return results, exception stack)
- ğŸ“ˆ Real-time task throughput, failure rate charts

**Start Flower:**

```bash
celery -A your_app flower --port=5555
```

**Alert Implementation with Prometheus + Grafana:**

1. Use `celery-exporter` to expose Celery data as Prometheus metrics
2. Configure Prometheus rules (e.g., task failures > threshold)
3. Set trigger conditions in Grafana (e.g., failure rate > 10% within 5 minutes) for alerts (email/Slack/Feishu/Webhook)

### 6. Multi-dimensional Rate Limiting & Security Protection

```mermaid
graph TD
    Request[API Request] --> SecurityLayer[Security Layer]

    subgraph Rate Limiting System
    SecurityLayer --> RateLimiter[Rate Limiter]
    RateLimiter --> UserLevel[User-level Rate Limit\n10/min]
    RateLimiter --> IPLevel[IP-level Rate Limit\n50/min]
    RateLimiter --> GlobalLevel[Global OpenAI Call Limit\n500/min]
    end

    subgraph Replay Attack Prevention
    SecurityLayer --> RequestFingerprint[Request Fingerprint Generation]
    RequestFingerprint --> BloomFilter[Bloom Filter]
    BloomFilter --> Cache[(Redis Cache)]
    BloomFilter --> CheckDuplicate{Duplicate?}
    CheckDuplicate -->|Yes| Reject[Reject Request]
    CheckDuplicate -->|No| Accept[Accept Request]
    end

    subgraph Data Security
    UserData[User Sensitive Data] --> ClientEncryption[Client-side AES-256 Encryption]
    ClientEncryption --> SecureTransmission[Encrypted Transmission]
    SecureTransmission --> ServerStorage[Server Stores Only Ciphertext]
    KeyManagement[Key Management] --> Vault[(HashiCorp Vault)]
    end
```

- User-level (10/min), IP-level (50/min), and global OpenAI call-level (500/min) rate limiting system built with Redis sliding window algorithm, intercepting 98% of high-frequency abuse requests
- Combined with Redis Bloom filter to cache processed request fingerprints, preventing duplicate submissions or replay attacks, with false positive rate < 0.1%

### 7. Multi-level Cache & High-frequency Interface Performance Optimization

```mermaid
graph TD
    Request[API Request] --> CacheRouter[Cache Router]

    subgraph Level 1 Cache
    CacheRouter --> RedisCache[Redis Cache]
    RedisCache --> L1Hit{Hit?}
    L1Hit -->|Yes| L1Return[Return Result]
    L1Hit -->|No| L2Cache

    RedisData1[High-frequency User Data] --> RedisCache
    RedisData2[Recent Interview Summaries] --> RedisCache
    RedisData3[Popular Question Embeddings] --> RedisCache
    end

    subgraph Level 2 Cache
    L2Cache[MongoDB Query Cache] --> L2Hit{Hit?}
    L2Hit -->|Yes| L2Return[Return Result]
    L2Hit -->|No| DatabaseQuery
    end

    subgraph Database Query
    DatabaseQuery[Execute MongoDB Query] --> StoreInCache
    StoreInCache[Store in Cache] --> L1Update[Update Redis Cache]
    StoreInCache --> L2Update[Update Query Cache]
    end

    subgraph Cache Strategy
    CachePolicy[Cache Policy] --> LRU[LRU Eviction Strategy]
    CachePolicy --> TTL[TTL Expiration Strategy]
    LRU --> EvictOldData[Evict Old Data]
    TTL --> AutoExpire[Auto Expire]
    end
```

- High-frequency data (e.g., user's last 3 interview summaries) using Redis cache with JSON serialization storage, reducing query latency from MongoDB's 15ms to 1ms
- Pre-generated and cached high-frequency interview question embeddings, reducing OpenAI API call volume by 35%
- Lightweight MongoDB query cache layer with LRU + TTL strategy for automatic old data eviction, reducing high-frequency query (e.g., user/position information) latency from 15ms to 1ms, increasing overall QPS by 4x

### 8. User Authentication & Data Security

- å‰ç«¯é›†æˆ Firebase Auth å®ç°èº«ä»½è®¤è¯ï¼Œåç«¯ç»“åˆ JWT ç®¡ç†æƒé™ä¸ç™»å½•æ€
- æ•æ„Ÿæ•°æ®ï¼ˆå¦‚ç”¨æˆ·ç®€å†ï¼‰ä½¿ç”¨å®¢æˆ·ç«¯ AES-256 åŠ å¯†ä¼ è¾“ï¼ŒæœåŠ¡ç«¯ä»…å­˜å‚¨å¯†æ–‡ï¼Œå¯†é’¥æ‰˜ç®¡åœ¨ HashiCorp Vault

## Performance Metrics & Optimization Results

```mermaid
graph LR
    subgraph Cache Performance Improvement
    MongoDB[MongoDB Query\n15ms] --> |After Optimization| RedisCache[Redis Cache\n1ms]
    APITokens[OpenAI API\nCall Volume] --> |After Optimization| APITokensReduced[OpenAI API\nCall Volume Reduced 35%]
    QPS[System QPS] --> |After Optimization| QPSIncreased[System QPS\nIncreased 4x]
    end

    subgraph Task Processing Optimization
    TaskDelay[Urgent Task Delay] --> |After Optimization| TaskDelayReduced[Urgent Task Delay\nReduced 60%]
    TokenCost[Token Cost] --> |After Optimization| TokenCostReduced[Token Cost\nReduced 45%]
    end

    subgraph Security Performance
    AbuseRequests[Abuse Requests] --> |Block Rate| BlockRate[Block 98%]
    FalsePositive[Bloom Filter\nFalse Positive Rate] --> FalsePositiveRate[<0.1%]
    end

    subgraph Retrieval Accuracy
    SearchAccuracy[Top-3 Retrieval\nResult Accuracy] --> AccuracyRate[92%]
    end
```

## Technical Highlights

1. **Real-time Audio-Video System**

   - Low-latency interview system built with WebRTC + Socket.IO + Redis Pub/Sub, supporting seamless switching between AI and human interviewers

2. **High-performance RAG Architecture**

   - Vector search engine based on LangChain + Qdrant, supporting Query-Answer vector similarity search

3. **Multi-level Cache Design**

   - Multi-level architecture with Redis + MongoDB query cache, reducing key query latency from 15ms to 1ms, increasing system QPS by 4x

4. **Asynchronous Task Optimization**

   - CPU/IO separation deployment strategy based on Celery, with priority scheduling, reducing urgent task latency by 60%

5. **Efficient Token Management**

   - Reducing overall token cost by 45% through summary caching and context truncation

6. **Comprehensive Security Protection**

   - Multi-dimensional rate limiting + Bloom filter + end-to-end encryption, blocking 98% of abuse requests with &lt;0.1% false positive rate

7. **Multi-Agent Routing System**

   - Context-aware dynamic routing based on intent classification, supporting switching between specialized domain agents

8. **Intelligent Email Analysis**
   - Gmail API integration + parallel async LLM processing + semantic extraction, building an intelligent tracking system for the entire application lifecycle

---

# MOCKI â€“ AI é©±åŠ¨çš„æ¨¡æ‹Ÿé¢è¯•å¹³å°

**æŠ€æœ¯æ ˆ**ï¼šReact + FastAPI + MongoDB + OpenAI + Qdrant + Redis + Celery + WebRTC + Firebase Auth + Resend

## ç³»ç»Ÿæ¶æ„

```mermaid
graph TD
    User[ç”¨æˆ·] --> Frontend[å‰ç«¯ React]
    Frontend --> WebRTC[WebRTC éŸ³è§†é¢‘]
    Frontend --> Websocket[Websocket å®æ—¶é€šä¿¡]
    Frontend <--> FirebaseAuth[Firebase Auth è®¤è¯]

    Websocket <--> NodeBackend[NestJS åç«¯]
    NodeBackend <--> RedisPS[Redis Pub/Sub]
    NodeBackend <--> PythonBackend[Python åç«¯]

    NodeBackend --> STT[è¯­éŸ³è½¬æ–‡æœ¬ STT]
    NodeBackend --> TTS[æ–‡æœ¬è½¬è¯­éŸ³ TTS]
    NodeBackend --> TranslationAPI[ç¿»è¯‘ API]

    PythonBackend --> OpenAI[OpenAI API]
    PythonBackend --> Qdrant[Qdrant å‘é‡æ•°æ®åº“]
    PythonBackend --> MongoDB[MongoDB]
    PythonBackend --> Redis[Redis ç¼“å­˜]
    PythonBackend --> CeleryWorkers[Celery Workers]

    CeleryWorkers --> Flower[Flower ç›‘æ§]
    Flower --> Prometheus[Prometheus]
    Prometheus --> Grafana[Grafana å¯è§†åŒ–]

    PythonBackend --> Gmail[Gmail API]
    PythonBackend --> Resend[Resend é‚®ä»¶æœåŠ¡]

    subgraph å®‰å…¨ç³»ç»Ÿ
    RateLimiter[Redis é™æµå™¨]
    BloomFilter[å¸ƒéš†è¿‡æ»¤å™¨]
    Vault[HashiCorp Vault]
    end

    PythonBackend --> å®‰å…¨ç³»ç»Ÿ
```

## ğŸŒŸ æ ¸å¿ƒåŠŸèƒ½ä¸ç³»ç»Ÿæ¨¡å—

### 1. AI é©±åŠ¨çš„æ¨¡æ‹Ÿé¢è¯•å¹³å°ï¼ˆæ ¸å¿ƒäº§å“èƒ½åŠ›ï¼‰

```mermaid
sequenceDiagram
    participant ç”¨æˆ·
    participant å‰ç«¯
    participant NestJS
    participant Pythonåç«¯
    participant OpenAI
    participant ç¼“å­˜ç³»ç»Ÿ

    ç”¨æˆ·->>å‰ç«¯: å‘èµ·é¢è¯•è¯·æ±‚
    å‰ç«¯->>NestJS: WebRTCéŸ³è§†é¢‘è¿æ¥
    å‰ç«¯->>NestJS: Websocketè¿æ¥
    NestJS->>Pythonåç«¯: å»ºç«‹Websocketè¿æ¥

    loop é¢è¯•è¿‡ç¨‹
        ç”¨æˆ·->>å‰ç«¯: éŸ³é¢‘è¾“å…¥
        å‰ç«¯->>NestJS: éŸ³é¢‘æµ
        NestJS->>NestJS: STTè½¬æ¢
        NestJS->>Pythonåç«¯: æ–‡æœ¬é—®é¢˜

        Pythonåç«¯->>ç¼“å­˜ç³»ç»Ÿ: æ£€æŸ¥é—®é¢˜ç¼“å­˜
        alt ç¼“å­˜å‘½ä¸­
            ç¼“å­˜ç³»ç»Ÿ-->>Pythonåç«¯: è¿”å›ç¼“å­˜ç»“æœ
        else ç¼“å­˜æœªå‘½ä¸­
            Pythonåç«¯->>OpenAI: LLMæ¨ç†è¯·æ±‚
            OpenAI-->>Pythonåç«¯: è¿”å›å›ç­”
            Pythonåç«¯->>ç¼“å­˜ç³»ç»Ÿ: æ›´æ–°ç¼“å­˜
        end

        Pythonåç«¯->>NestJS: è¿”å›AIå›ç­”
        NestJS->>NestJS: TTSè½¬æ¢
        NestJS->>å‰ç«¯: éŸ³é¢‘å›ç­”
        å‰ç«¯->>ç”¨æˆ·: æ’­æ”¾AIå›ç­”
    end

    ç”¨æˆ·->>å‰ç«¯: ç»“æŸé¢è¯•
    å‰ç«¯->>NestJS: å…³é—­è¿æ¥
    NestJS->>Pythonåç«¯: ä¿å­˜é¢è¯•è®°å½•
```

- ç‹¬ç«‹å¼€å‘å®Œæ•´é“¾è·¯ï¼šéŸ³è§†é¢‘é¢è¯•æµç¨‹åŸºäº WebRTC â†’ STT â†’ LLM æ¨ç† â†’ TTS è¿”å›éŸ³é¢‘ï¼Œæ”¯æŒ AI é¢è¯•å®˜ä¸çœŸäººé¢è¯•å®˜è‡ªç”±åˆ‡æ¢
- æ„å»ºåŸºäº Socket.IO + Redis Pub/Sub + Redis Adapter çš„å®æ—¶é€šä¿¡ç³»ç»Ÿï¼Œæ”¯æŒå¤šå®¢æˆ·ç«¯ã€å¤šå®ä¾‹éƒ¨ç½²ä¸‹çš„æ¶ˆæ¯åŒæ­¥ä¸çŠ¶æ€å¹¿æ’­
- æ”¯æŒè§’è‰²åˆ‡æ¢ï¼ˆç³»ç»Ÿã€AIã€çœŸäººï¼‰ã€é—®é¢˜åº“ç®¡ç†ã€ç®€å†å¯¼å…¥ä¸èŒä½å®šå‘é—®ç­”ï¼Œæ„å»ºç»“æ„åŒ–ä¸éç»“æ„åŒ–é—®é¢˜ç»„åˆ
- ç³»ç»Ÿæ¶æ„ï¼š
  - NestJSåç«¯ï¼šè´Ÿè´£è¿æ¥éŸ³è§†é¢‘é€šè¯å’Œå¯¹è¯è®°å½•ï¼ŒHTTPè°ƒç”¨ç¿»è¯‘APIå’ŒTTS APIï¼Œæµå¼STTï¼ˆç™¾åº¦ã€è°·æ­Œã€è®¯é£ï¼‰ï¼Œç»´æŒä¸å‰ç«¯çš„websocketè¿æ¥ï¼ŒåŒæ—¶ä¸Pythonåç«¯ä¿æŒwebsocketè¿æ¥
  - Pythonåç«¯ï¼šå¤„ç†AIè¯·æ±‚ï¼ŒRAGç³»ç»Ÿï¼Œé‚®ä»¶åˆ†æç³»ç»Ÿï¼Œç®¡ç†ç”¨æˆ·ç™»å½•ï¼Œå®ç°é™æµã€‚åœ¨AIé¢è¯•ç³»ç»Ÿä¸­ä¸nodejsé€šè¿‡websocketè¿æ¥ï¼Œåœ¨ç™»å½•ã€é‚®ä»¶åˆ†æç³»ç»Ÿä¸å‰ç«¯é€šè¿‡REST APIäº¤äº’

### 2. é‚®ä»¶æ™ºèƒ½åˆ†æç³»ç»Ÿï¼ˆAI ä¿¡æ¯æŠ½å– + è‡ªåŠ¨é—­ç¯ï¼‰

```mermaid
graph TD
    Start[ç”¨æˆ·æˆæƒ] --> OAuth[Gmail OAuth2.0è®¤è¯]
    OAuth --> FetchEmails[æ‹‰å–æŠ•é€’é‚®ä»¶]
    FetchEmails --> Filter[æŒ‰æ ‡ç­¾/æ—¶é—´è¿‡æ»¤]

    Filter --> ParallelProcessing[å¹¶è¡Œå¼‚æ­¥å¤„ç†]
    ParallelProcessing --> CeleryTasks[Celeryä»»åŠ¡åˆ†é…]

    CeleryTasks --> AnalyzeEmail1[åˆ†æé‚®ä»¶1]
    CeleryTasks --> AnalyzeEmail2[åˆ†æé‚®ä»¶2]
    CeleryTasks --> AnalyzeEmailN[åˆ†æé‚®ä»¶N...]

    AnalyzeEmail1 --> LLMProcessing[LLMä¿¡æ¯æå–]
    AnalyzeEmail2 --> LLMProcessing
    AnalyzeEmailN --> LLMProcessing

    LLMProcessing --> ExtractStatus[æå–ç”³è¯·çŠ¶æ€]

    ExtractStatus --> CreateTimeline[ç”Ÿæˆæ—¶é—´çº¿]
    CreateTimeline --> GenerateTasks[ç”Ÿæˆå¾…åŠäº‹é¡¹]
    CreateTimeline --> JobRecommendation[èŒä½æ¨è]

    GenerateTasks --> Notification[å‘é€é€šçŸ¥]
    JobRecommendation --> Notification
```

- æ”¯æŒç”¨æˆ·é€šè¿‡ OAuth2.0 ç™»å½• Gmailï¼Œä½¿ç”¨ Gmail API æ‹‰å–æŠ•é€’é‚®ä»¶ï¼ŒæŒ‰æ ‡ç­¾/æ—¶é—´è¿‡æ»¤å†å²é‚®ä»¶
- ä½¿ç”¨å¹¶è¡Œå¼‚æ­¥ LLM åˆ†æé‚®ä»¶å†…å®¹ï¼Œæå–ç”³è¯·çŠ¶æ€ï¼ˆå¦‚å·²è¯»ã€å·²å›å¤ã€é¢è¯•é‚€è¯·ï¼‰ã€èŒä½åç§°ã€å…¬å¸ä¿¡æ¯ç­‰å­—æ®µï¼Œå¹¶èšåˆå½¢æˆ Timeline
- æ•´åˆå…¬å¸èŒä½ä¿¡æ¯ï¼Œå‘ç”¨æˆ·æ¨é€èŒä½åˆ—è¡¨æ¨è

### 3. å¾…åŠäº‹é¡¹ç®¡ç†ä¸æ™ºèƒ½æé†’ç³»ç»Ÿ

- è‡ªåŠ¨æ ¹æ®é‚®ä»¶è§£æä¸ç”¨æˆ·è¡Œä¸ºç”Ÿæˆå¾…åŠäº‹é¡¹ï¼Œ"TODO" "DONE"ç­‰ï¼Œæ„å»ºå®Œæ•´æŠ•é€’è·Ÿè¸ªè·¯å¾„
- ä½¿ç”¨ Resend å®ç°åŸºäºæ—¶é—´è§¦å‘çš„ä¸´æœŸæé†’ã€å®æ—¶é€šçŸ¥ä¸çŠ¶æ€å˜æ›´æ¨é€ï¼Œé¿å…å…³é”®ä¿¡æ¯é—æ¼
- æ”¯æŒçŠ¶æ€åŒæ­¥ä¸è·¨è®¾å¤‡è®¿é—®ï¼Œç”¨æˆ·æ“ä½œå³æ—¶åé¦ˆè‡³æ•°æ®åº“

### 4. RAG ç³»ç»Ÿä¸ä¸Šä¸‹æ–‡é—®ç­”èƒ½åŠ›ä¼˜åŒ–

```mermaid
graph TD
    subgraph Index Construction
    Resume[ç”¨æˆ·ç®€å†] --> TextExtractor[æ–‡æœ¬æå–]
    JobDesc[èŒä½æè¿°] --> TextExtractor
    TextExtractor --> Chunker[æ–‡æœ¬åˆ†å—]

    Chunker --> QAPairCreator[Query-Answerå¯¹ç”Ÿæˆ]
    QAPairCreator --> QueryEmbedder[QueryåµŒå…¥ç”Ÿæˆ]
    QAPairCreator --> AnswerEmbedder[AnsweråµŒå…¥ç”Ÿæˆ]

    QueryEmbedder --> VectorStore[Qdrantå‘é‡å­˜å‚¨]
    AnswerEmbedder --> VectorStore
    QAPairCreator --> MetadataStore[MongoDBå…ƒæ•°æ®]
    end

    subgraph Retrieval Process
    Query[ç”¨æˆ·é—®é¢˜] --> InputEmbedder[æŸ¥è¯¢åµŒå…¥ç”Ÿæˆ]

    InputEmbedder --> QuerySimilarity[Queryç›¸ä¼¼åº¦æ£€ç´¢]
    InputEmbedder --> AnswerSimilarity[Answerç›¸ä¼¼åº¦æ£€ç´¢]

    QuerySimilarity --> Qdrant[(Qdrant\nå‘é‡æ•°æ®åº“)]
    AnswerSimilarity --> Qdrant

    QuerySimilarity --> WeightedMerger[åŠ æƒç»“æœåˆå¹¶]
    AnswerSimilarity --> WeightedMerger
    WeightedMerger --> DocumentIDs[è¿”å›ç›¸ä¼¼æ–‡æ¡£ID]

    DocumentIDs --> MongoDB[(MongoDB\nå…ƒæ•°æ®å­˜å‚¨)]
    MongoDB --> RetrievedDocs[æ£€ç´¢å®Œæ•´æ–‡æ¡£]
    end

    subgraph Context Management
    RetrievedDocs --> ContextFormatter[ä¸Šä¸‹æ–‡æ ¼å¼åŒ–]
    ContextFormatter --> TokenCounter[Tokenè®¡æ•°å™¨]

    PreviousQA[å†å²å¯¹è¯] --> SummaryBuffer[æ‘˜è¦ç¼“å­˜]
    SummaryBuffer --> TokenThreshold[Tokené˜ˆå€¼æ§åˆ¶]
    TokenThreshold --> ContextTruncation[ä¸Šä¸‹æ–‡æˆªæ–­]
    end

    subgraph Multi-Agent Routing
    Query --> IntentClassifier[æ„å›¾åˆ†ç±»å™¨]
    IntentClassifier --> MemoryRouter[è®°å¿†è·¯ç”±å™¨]
    MemoryRouter --> BQAgent[BQé¢è¯•Agent]
    MemoryRouter --> CodeAgent[ä»£ç é¢è¯•Agent]
    MemoryRouter --> KnowledgeAgent[åŸºç¡€çŸ¥è¯†Agent]
    MemoryRouter --> AnalysisAgent[é¢è¯•åˆ†æAgent]
    MemoryRouter --> QuestionAgent[æé—®Agent]
    end

    ContextFormatter --> LLMPrompt[LLMæç¤ºæ„å»º]
    ContextTruncation --> LLMPrompt
    MemoryRouter --> LLMPrompt
    LLMPrompt --> OpenAI[OpenAI API]
    OpenAI --> Response[ç”Ÿæˆå“åº”]
```

- Vector search engine based on LangChain + Qdrant, supporting Query-Answer vector similarity search
- Efficient retrieval process: user questions are vectorized and simultaneously compared with pre-processed Query and Answer vectors for similarity calculation, with weighted merging of results to obtain the most relevant content
- Optimized document processing: interview materials are pre-processed into Query-Answer pairs, with separate embedding calculations, enabling retrieval to consider both question similarity and answer relevance
- Simplified index structure: direct vector similarity sorting for retrieval, avoiding complex index algorithm tuning while meeting performance requirements at current data scale
- Multi-turn Q&A using ConversationSummaryBufferMemory for summary caching, combined with Token threshold controller for automatic context truncation, reducing overall Token cost by 45%
- Intent classification using LangChain LLMChain, driving MemoryRouter for dynamic context switching between multiple Agents
- Professional Agent system includes:
  - BQ Interview Agent: handling behavioral questions and soft skills assessment
  - Coding Interview Agent: evaluating programming ability and algorithm analysis
  - Basic Knowledge Agent: testing professional domain knowledge
  - Interview Analysis Agent: providing interview performance evaluation and feedback
  - Question Agent: generating in-depth questions based on candidate background

### 5. å¼‚æ­¥ä»»åŠ¡ç³»ç»Ÿï¼ˆæ€§èƒ½ä¿éšœèƒ½åŠ›ï¼‰

```mermaid
graph LR
    subgraph Task Generation
    WebRequest[Web Request] --> TaskRouter
    ScheduledJob[Scheduled Job] --> TaskRouter
    EmailAnalysis[Email Analysis] --> TaskRouter
    end

    subgraph Task Routing
    TaskRouter[Task Router] --> PriorityQueue
    TaskRouter --> UserPriority{User Priority}
    UserPriority -->|VIP User| HighPriority[High Priority Queue]
    UserPriority -->|Regular User| NormalPriority[Normal Priority Queue]
    end

    subgraph Queue System
    PriorityQueue[Priority Queue] --> Redis[(Redis)]
    HighPriority --> Redis
    NormalPriority --> Redis
    end

    subgraph Worker Pool
    Redis --> Dispatcher[Task Dispatcher]
    Dispatcher --> CPUWorkers[CPU-intensive Workers\nPDF/Email Parsing]
    Dispatcher --> IOWorkers[IO-intensive Workers\nLLM Calls/Embedding Generation]
    end

    subgraph Monitoring System
    CPUWorkers --> Flower[Flower Monitoring]
    IOWorkers --> Flower
    Flower --> Prometheus[Prometheus]
    Prometheus --> Grafana[Grafana]
    Grafana --> Alerts[Alert Notifications]
    end
```

- Multi-machine multi-queue task scheduling system based on Celery + Redis, with Worker pool independently deployed for CPU-intensive (PDF/email parsing) and IO-intensive (LLM calls, embedding generation) tasks
- Priority task scheduling (e.g., VIP user request queue jumping), reducing urgent task average latency by 60%
- Real-time monitoring with Flower, combined with Prometheus/Grafana for automatic alerts and visual tracking of task failure rates and queue backlog metrics

#### Flower Monitoring Tool Details

Flower is Celery's officially recommended task monitoring tool, suitable for **real-time monitoring of Celery task execution, Worker status, task queue backlog, etc.**. It can be understood as Celery's visual Dashboard + API server.

**Flower Features:**

- ğŸ‘¨â€ğŸ”§ Worker status (active, stopped, heartbeat lost)
- ğŸ“¦ Queue length (backlog status)
- âœ… Task execution status (success, failure, retry, duration)
- ğŸ§© Task details (input parameters, return results, exception stack)
- ğŸ“ˆ Real-time task throughput, failure rate charts

**Start Flower:**

```bash
celery -A your_app flower --port=5555
```

**Alert Implementation with Prometheus + Grafana:**

1. Use `celery-exporter` to expose Celery data as Prometheus metrics
2. Configure Prometheus rules (e.g., task failures > threshold)
3. Set trigger conditions in Grafana (e.g., failure rate > 10% within 5 minutes) for alerts (email/Slack/Feishu/Webhook)

### 6. å¤šç»´é™æµä¸å®‰å…¨é˜²æŠ¤æœºåˆ¶

```mermaid
graph TD
    Request[API Request] --> SecurityLayer[Security Layer]

    subgraph Rate Limiting System
    SecurityLayer --> RateLimiter[Rate Limiter]
    RateLimiter --> UserLevel[User-level Rate Limit\n10/min]
    RateLimiter --> IPLevel[IP-level Rate Limit\n50/min]
    RateLimiter --> GlobalLevel[Global OpenAI Call Limit\n500/min]
    end

    subgraph Replay Attack Prevention
    SecurityLayer --> RequestFingerprint[Request Fingerprint Generation]
    RequestFingerprint --> BloomFilter[Bloom Filter]
    BloomFilter --> Cache[(Redis Cache)]
    BloomFilter --> CheckDuplicate{Duplicate?}
    CheckDuplicate -->|Yes| Reject[Reject Request]
    CheckDuplicate -->|No| Accept[Accept Request]
    end

    subgraph Data Security
    UserData[User Sensitive Data] --> ClientEncryption[Client-side AES-256 Encryption]
    ClientEncryption --> SecureTransmission[Encrypted Transmission]
    SecureTransmission --> ServerStorage[Server Stores Only Ciphertext]
    KeyManagement[Key Management] --> Vault[(HashiCorp Vault)]
    end
```

- User-level (10/min), IP-level (50/min), and global OpenAI call-level (500/min) rate limiting system built with Redis sliding window algorithm, intercepting 98% of high-frequency abuse requests
- Combined with Redis Bloom filter to cache processed request fingerprints, preventing duplicate submissions or replay attacks, with false positive rate < 0.1%

### 7. å¤šçº§ç¼“å­˜ä¸é«˜é¢‘æ¥å£æ€§èƒ½ä¼˜åŒ–

```mermaid
graph TD
    Request[API Request] --> CacheRouter[Cache Router]

    subgraph Level 1 Cache
    CacheRouter --> RedisCache[Redis Cache]
    RedisCache --> L1Hit{Hit?}
    L1Hit -->|Yes| L1Return[Return Result]
    L1Hit -->|No| L2Cache

    RedisData1[High-frequency User Data] --> RedisCache
    RedisData2[Recent Interview Summaries] --> RedisCache
    RedisData3[Popular Question Embeddings] --> RedisCache
    end

    subgraph Level 2 Cache
    L2Cache[MongoDB Query Cache] --> L2Hit{Hit?}
    L2Hit -->|Yes| L2Return[Return Result]
    L2Hit -->|No| DatabaseQuery
    end

    subgraph Database Query
    DatabaseQuery[Execute MongoDB Query] --> StoreInCache
    StoreInCache[Store in Cache] --> L1Update[Update Redis Cache]
    StoreInCache --> L2Update[Update Query Cache]
    end

    subgraph Cache Strategy
    CachePolicy[Cache Policy] --> LRU[LRU Eviction Strategy]
    CachePolicy --> TTL[TTL Expiration Strategy]
    LRU --> EvictOldData[Evict Old Data]
    TTL --> AutoExpire[Auto Expire]
    end
```

- High-frequency data (e.g., user's last 3 interview summaries) using Redis cache with JSON serialization storage, reducing query latency from MongoDB's 15ms to 1ms
- Pre-generated and cached high-frequency interview question embeddings, reducing OpenAI API call volume by 35%
- Lightweight MongoDB query cache layer with LRU + TTL strategy for automatic old data eviction, reducing high-frequency query (e.g., user/position information) latency from 15ms to 1ms, increasing overall QPS by 4x

### 8. ç”¨æˆ·è®¤è¯ä¸æ•°æ®å®‰å…¨

- å‰ç«¯é›†æˆ Firebase Auth å®ç°èº«ä»½è®¤è¯ï¼Œåç«¯ç»“åˆ JWT ç®¡ç†æƒé™ä¸ç™»å½•æ€
- æ•æ„Ÿæ•°æ®ï¼ˆå¦‚ç”¨æˆ·ç®€å†ï¼‰ä½¿ç”¨å®¢æˆ·ç«¯ AES-256 åŠ å¯†ä¼ è¾“ï¼ŒæœåŠ¡ç«¯ä»…å­˜å‚¨å¯†æ–‡ï¼Œå¯†é’¥æ‰˜ç®¡åœ¨ HashiCorp Vault

## æ€§èƒ½æŒ‡æ ‡ä¸ä¼˜åŒ–æ•ˆæœ

```mermaid
graph LR
    subgraph Cache Performance Improvement
    MongoDB[MongoDB Query\n15ms] --> |After Optimization| RedisCache[Redis Cache\n1ms]
    APITokens[OpenAI API\nCall Volume] --> |After Optimization| APITokensReduced[OpenAI API\nCall Volume Reduced 35%]
    QPS[System QPS] --> |After Optimization| QPSIncreased[System QPS\nIncreased 4x]
    end

    subgraph Task Processing Optimization
    TaskDelay[Urgent Task Delay] --> |After Optimization| TaskDelayReduced[Urgent Task Delay\nReduced 60%]
    TokenCost[Token Cost] --> |After Optimization| TokenCostReduced[Token Cost\nReduced 45%]
    end

    subgraph Security Performance
    AbuseRequests[Abuse Requests] --> |Block Rate| BlockRate[Block 98%]
    FalsePositive[Bloom Filter\nFalse Positive Rate] --> FalsePositiveRate[<0.1%]
    end

    subgraph Retrieval Accuracy
    SearchAccuracy[Top-3 Retrieval\nResult Accuracy] --> AccuracyRate[92%]
    end
```

## æŠ€æœ¯äº®ç‚¹æ€»ç»“

1. **å®æ—¶éŸ³è§†é¢‘äº¤äº’ç³»ç»Ÿ**

   - WebRTC + Socket.IO + Redis Pub/Sub æ„å»ºçš„ä½å»¶è¿Ÿé¢è¯•ç³»ç»Ÿï¼Œæ”¯æŒAIä¸çœŸäººæ— ç¼åˆ‡æ¢

2. **é«˜æ€§èƒ½RAGæ¶æ„**

   - Vector search engine based on LangChain + Qdrant, supporting Query-Answer vector similarity search

3. **å¤šçº§ç¼“å­˜è®¾è®¡**

   - Redis + MongoDBæŸ¥è¯¢ç¼“å­˜çš„å¤šçº§æ¶æ„ï¼Œå°†å…³é”®æŸ¥è¯¢å»¶è¿Ÿä»15msé™è‡³1msï¼Œç³»ç»ŸQPSæå‡4å€

4. **å¼‚æ­¥ä»»åŠ¡ä¼˜åŒ–**

   - åŸºäºCeleryçš„CPU/IOåˆ†ç¦»éƒ¨ç½²ç­–ç•¥ï¼Œç»“åˆä¼˜å…ˆçº§è°ƒåº¦ï¼Œç´§æ€¥ä»»åŠ¡å»¶è¿Ÿé™ä½60%

5. **é«˜æ•ˆTokenç®¡ç†**

   - é€šè¿‡æ‘˜è¦ç¼“å­˜ä¸ä¸Šä¸‹æ–‡æˆªæ–­æŠ€æœ¯ï¼Œæ•´ä½“Tokenæˆæœ¬é™ä½45%

6. **å…¨é¢çš„å®‰å…¨é˜²æŠ¤**

   - å¤šç»´é™æµ + å¸ƒéš†è¿‡æ»¤å™¨ + ç«¯åˆ°ç«¯åŠ å¯†ï¼Œæ‹¦æˆª98%æ»¥ç”¨è¯·æ±‚ï¼Œè¯¯åˆ¤ç‡&lt;0.1%

7. **å¤šAgentè·¯ç”±ç³»ç»Ÿ**

   - åŸºäºæ„å›¾åˆ†ç±»çš„ä¸Šä¸‹æ–‡åŠ¨æ€è·¯ç”±ï¼Œæ”¯æŒåœ¨ä¸åŒä¸“ä¸šé¢†åŸŸAgenté—´åˆ‡æ¢

8. **é‚®ä»¶æ™ºèƒ½åˆ†æ**
   - Gmail APIé›†æˆ + å¹¶è¡Œå¼‚æ­¥LLMå¤„ç† + è¯­ä¹‰æå–ï¼Œæ„å»ºæŠ•é€’å…¨ç”Ÿå‘½å‘¨æœŸçš„æ™ºèƒ½è·Ÿè¸ªç³»ç»Ÿ
